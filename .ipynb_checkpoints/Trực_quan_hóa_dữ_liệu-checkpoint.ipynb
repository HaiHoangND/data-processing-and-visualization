{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQDdKP8OjLxQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMWiicWhjLAE"
   },
   "source": [
    "**Các column của bảng quản lý nhà đất: address,price, area,category, extensions, project_legal(sổ hồng, sổ đỏ...), house_direction, posting_date, expire_date, average_price/1m2, population_density, AI_predicted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNHiumIGjNTk"
   },
   "source": [
    "Trang homedy.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lCD2YmMRFsq",
    "outputId": "48ca5ace-6826-4a88-e49b-6d7be3c214d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p95: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p94: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p102: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p101: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p99: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p100: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p93: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p98: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p96: 524\n",
      "Yêu cầu không thành công cho trang https://homedy.com/ban-nha-dat-ha-noi/p97: 524\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def append_to_json(data):\n",
    "    filename = \"/content/drive/MyDrive/Colab Notebooks/Data_homedy.json\"\n",
    "    with lock:  # Sử dụng lock để đảm bảo chỉ một luồng có thể ghi vào tệp JSON tại một thời điểm\n",
    "        with open(filename, 'a', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def get_scrapeops_data(url):\n",
    "    response = requests.get(f'https://homedy.com{url}')\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div_root = soup.find('div', class_='content')\n",
    "        address = target_div_root.find('div',class_ ='address')\n",
    "        description = target_div_root.find('div',class_ = 'description readmore')\n",
    "        price_area = target_div_root.find('div',class_='product-short-info')\n",
    "        date_info = target_div_root.find('div',class_='product-info')\n",
    "        posting_date = date_info.find_all('p', class_='code')[0].text.strip()\n",
    "        expire_date = date_info.find_all('p', class_='code')[1].text.strip()\n",
    "        status = date_info.find_all('p', class_='code')[2].text.strip()\n",
    "        price_info = price_area.find('div', class_='short-item').find('strong').text.strip()\n",
    "        area_info = price_area.find_all('div', class_='short-item')[1].find('strong').text.strip()\n",
    "        address_parts = [span.get_text(strip=True) for span in address.find_all('span')]\n",
    "        data = {\n",
    "            \"title\": address.find('a').get_text(strip=True) if address else None,\n",
    "            \"address\": ', '.join(address_parts),\n",
    "            \"description\": description.text.strip() if description else None,\n",
    "            \"price\": price_info,\n",
    "            'area': area_info ,\n",
    "            'posting_date': posting_date,\n",
    "            'expire_date': expire_date,\n",
    "            'status': status\n",
    "        }\n",
    "        return data\n",
    "\n",
    "def process_page(page_number):\n",
    "    url = f'https://homedy.com/ban-nha-dat-ha-noi/p{page_number}'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_links = soup.find_all('a', class_='thumb-image', href=True)\n",
    "        all_data = []\n",
    "        for link in target_links:\n",
    "           data = get_scrapeops_data(link['href'])\n",
    "           all_data.append(data)\n",
    "        append_to_json(all_data)\n",
    "    else:\n",
    "        print(f\"Yêu cầu không thành công cho trang {url}: {response.status_code}\")\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        executor.map(process_page, range(1, 200))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wji61_27jFF6"
   },
   "source": [
    "Trang dothi.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goapW20UXfJ3"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def append_to_json(data):\n",
    "    filename = \"/content/drive/MyDrive/Colab Notebooks/Data_dothi.json\"\n",
    "    with lock:  # Sử dụng lock để đảm bảo chỉ một luồng có thể ghi vào tệp JSON tại một thời điểm\n",
    "        with open(filename, 'a', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def get_scrapeops_data(url):\n",
    "    response = requests.get(f'https://dothi.net{url}')\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            target_div_root = soup.find('div', class_='product-detail')\n",
    "            address = target_div_root.find('div', class_='pd-location')\n",
    "            description = target_div_root.find('div', class_='pd-desc')\n",
    "            price_area = target_div_root.find('div', class_='pd-price')\n",
    "            price_info = price_area.find('span', class_='spanprice').text.strip()\n",
    "\n",
    "            # Trích xuất thông tin diện tích\n",
    "            area_info = price_area.find_all('span')[1].text.strip()\n",
    "            detail_div = target_div_root.find('div', class_='pd-dacdiem')\n",
    "            detail_table = detail_div.find('table')\n",
    "            rows = detail_table.find_all('tr')\n",
    "            data = {}\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    key = cells[0].text.strip()\n",
    "                    value = cells[1].text.strip()\n",
    "                    data[key] = value\n",
    "\n",
    "            posting_date = data.get('Ngày đăng tin')\n",
    "            expire_date = data.get('Ngày hết hạn')\n",
    "            status = None  # You need to fill this according to your source\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "        data = {\n",
    "            \"title\": address.find('a').get_text(strip=True) if address else None,\n",
    "            \"address\": address.get_text().split('-', 1)[1].strip() if '-' in address.get_text() else '',\n",
    "            \"description\": description.text.strip() if description else None,\n",
    "            \"price\": price_info,\n",
    "            'area': area_info,\n",
    "            'posting_date': posting_date,\n",
    "            'expire_date': expire_date,\n",
    "            'status': status\n",
    "        }\n",
    "        return data\n",
    "\n",
    "def process_page(page_number):\n",
    "    url = f'https://dothi.net/nha-dat-ban-ha-noi/p{page_number}.htm'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_links = soup.select('a.vip2, a.vip5')\n",
    "        all_data = []\n",
    "        for link in target_links:\n",
    "            data = get_scrapeops_data(link['href'])\n",
    "            all_data.append(data)\n",
    "        append_to_json(all_data)\n",
    "    else:\n",
    "        print(f\"Yêu cầu không thành công cho trang {url}: {response.status_code}\")\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:  # Số luồng tối đa là 100\n",
    "        executor.map(process_page, range(1, 400))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
